{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 182,
     "status": "ok",
     "timestamp": 1676779931682,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "IkadpoeBdSgU"
   },
   "outputs": [],
   "source": [
    "# %%shell\n",
    "# jupyter nbconvert --to html /content/HW1_CSCI544.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1864,
     "status": "ok",
     "timestamp": 1676779933998,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "Va_li5b7j4nG",
    "outputId": "294ac411-3596-44f9-e8e1-b089edb85e18"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.metrics.scores import precision\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "# from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1676779934776,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "QiPJKAkkj4nJ"
   },
   "outputs": [],
   "source": [
    "# ! pip install bs4 # in case you don't have it installed\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKWDGn15j4nN"
   },
   "source": [
    "### Helper Function (Length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1676779935555,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "lK8xn99HcKKz"
   },
   "outputs": [],
   "source": [
    "# Average Length Function\n",
    "def average_length(review_column) -> str:\n",
    "  return review_column.apply(len).mean()\n",
    "\n",
    "def average_length_print(before_cleaning, review_column) -> str:\n",
    "  return str(before_cleaning) + \", \" + str(review_column.apply(len).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgxNGX_Ubqby"
   },
   "source": [
    "# 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsn6oknhj4nK"
   },
   "source": [
    "### Read Data\n",
    "Imported the data as a Pandas frame using Pandas package and only kept the Reviews and Ratings fields in the input data frame to generate data. <br>\n",
    "Reference - https://www.geeksforgeeks.org/how-to-load-a-tsv-file-into-a-pandas-dataframe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65343,
     "status": "ok",
     "timestamp": 1676777607673,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "TVu4NQDKj4nL",
    "outputId": "729287d4-036b-4a44-f9f7-ba15d90b5c6e"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_table('data.tsv', on_bad_lines='skip', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropped the column with the null value and converted the datatype of star_rating column to int for consistency. <br>\n",
    "Reference - https://www.statology.org/pandas-create-dataframe-from-existing-dataframe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 48856,
     "status": "ok",
     "timestamp": 1676777656518,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "GDyzLJc6j4nM"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Love this, excellent sun block!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The great thing about this cream is that it do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Great Product, I'm 65 years old and this is al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>I use them as shower caps &amp; conditioning caps....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>This is my go-to daily sunblock. It leaves no ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  star_rating                                        review_body\n",
       "0           5                   Love this, excellent sun block!!\n",
       "1           5  The great thing about this cream is that it do...\n",
       "2           5  Great Product, I'm 65 years old and this is al...\n",
       "3           5  I use them as shower caps & conditioning caps....\n",
       "4           5  This is my go-to daily sunblock. It leaves no ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputDataFrame = dataset[['star_rating','review_body']].copy()\n",
    "InputDataFrame.to_csv('Dataset_Input_Data_Frame.csv', index=False)\n",
    "InputDataFrame = pd.read_csv('Dataset_Input_Data_Frame.csv', low_memory=False)\n",
    "InputDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1088,
     "status": "ok",
     "timestamp": 1676777678799,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "dH3iS0ZISXC_",
    "outputId": "790a5d64-3d67-47a0-d931-38abaf901f1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "star_rating     10\n",
       "review_body    400\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputDataFrame.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1891,
     "status": "ok",
     "timestamp": 1676777680687,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "tEAQcNUDIhm8"
   },
   "outputs": [],
   "source": [
    "# InputDataFrame.info(verbose = True, show_counts = True)\n",
    "InputDataFrame.dropna(inplace = True)\n",
    "\n",
    "InputDataFrame['star_rating'] = InputDataFrame['star_rating'].astype('float').astype('int')\n",
    "# InputDataFrame.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1676777681118,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "21Bn1GOSScG3",
    "outputId": "aa5229e2-f261-45e8-fb72-183ad292f0f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "star_rating    0\n",
       "review_body    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputDataFrame.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a three-class classification problem according to ratings with ratings with the values of 1 and 2 from class 0, ratings with the value of 3 form class 1, and \n",
    "ratings with the values of 4 and 5 form class 2. <br>\n",
    "Reference - https://www.geeksforgeeks.org/create-a-new-column-in-pandas-dataframe-based-on-the-existing-columns/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 2420,
     "status": "ok",
     "timestamp": 1676777683532,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "fOacG3kXYnqY",
    "outputId": "ee58ceb3-53aa-4f07-ca59-92a4381a44de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Love this, excellent sun block!!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The great thing about this cream is that it do...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Great Product, I'm 65 years old and this is al...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>I use them as shower caps &amp; conditioning caps....</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>This is my go-to daily sunblock. It leaves no ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   star_rating                                        review_body  class\n",
       "0            5                   Love this, excellent sun block!!      2\n",
       "1            5  The great thing about this cream is that it do...      2\n",
       "2            5  Great Product, I'm 65 years old and this is al...      2\n",
       "3            5  I use them as shower caps & conditioning caps....      2\n",
       "4            5  This is my go-to daily sunblock. It leaves no ...      2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assign_class(value):\n",
    "    if value == 1 or value == 2:\n",
    "        return 0\n",
    "    if value == 3:\n",
    "        return 1\n",
    "    if value == 4 or value == 5:\n",
    "        return 2\n",
    " \n",
    "InputDataFrame['class'] = InputDataFrame['star_rating'].map(assign_class)\n",
    "\n",
    "# InputDataFrame['class'].value_counts()\n",
    "# display(InputDataFrame)\n",
    "InputDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1676777685910,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "DRmloMrInJLO",
    "outputId": "d8e2df76-68d3-4555-e41a-24ffdbd74019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Average Length : 253.43061308343476\n"
     ]
    }
   ],
   "source": [
    "before_cleaning = average_length(InputDataFrame[\"review_body\"])\n",
    "print(\"Review Average Length : \" + str(before_cleaning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InputDataFrame.to_csv('Dataset_Data_Frame.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputDataFrameHW1 = InputDataFrame.copy()\n",
    "InputDataFrameHW3 = InputDataFrame.copy()\n",
    "\n",
    "# InputDataFrameHW1 = pd.read_csv('Dataset_Data_Frame.csv')\n",
    "# InputDataFrameHW3 = pd.read_csv('Dataset_Data_Frame.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    3979156\n",
       "0     717991\n",
       "1     396760\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputDataFrameHW1['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset, InputDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function Class (Data Cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning steps to preprocess the dataset you created\n",
    "####  Contraction()\n",
    "code to perform contractions on the reviews where different regexes are used to decontract the words in the review. Two general contraction regex are used and 8 specific regexes for contraction are used.  \n",
    "\n",
    "#### Lowercase()\n",
    "code to convert all reviews into lowercase where each word of the data frame is lowered with the help of python function .lower()\n",
    "\n",
    "####  RemoveHTMLURL() \n",
    "code to remove the HTML and URLs from the reviews where two different regex is used to remove the html tag and the url by passing each review text to html_url function \n",
    "\n",
    "####  RemoveNonAlphabeticalCharacter() \n",
    "code to remove non-alphabetical characters where single regex expression which remove the characters except the a-z, A-Z and spaces between words \n",
    "\n",
    "####  RemoveExtraSpaces() \n",
    "code to remove extra spaces where regex is used to remove extra white spaces<br>\n",
    "\n",
    "Reference - https://www.kaggle.com/code/benroshan/sentiment-analysis-amazon-reviews/notebook <br>\n",
    "Reference - https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 170,
     "status": "ok",
     "timestamp": 1676777688920,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "5TYbwjx8dztw"
   },
   "outputs": [],
   "source": [
    "class Contraction():\n",
    "    def __init__(self, InputDataFrame) -> None:\n",
    "        self.InputDataFrame = InputDataFrame\n",
    "        \n",
    "    def contraction_function(self, review):\n",
    "        review = re.sub(r\"won\\'t\", \"will not\", review)\n",
    "        review = re.sub(r\"can\\'t\", \"can not\", review)\n",
    "        review = re.sub(r\"n\\'t\", \" not\", review)\n",
    "        review = re.sub(r\"\\'re\", \" are\", review)\n",
    "        review = re.sub(r\"\\'s\", \" is\", review)\n",
    "        review = re.sub(r\"\\'d\", \" would\", review)\n",
    "        review = re.sub(r\"\\'ll\", \" will\", review)\n",
    "        review = re.sub(r\"\\'t\", \" not\", review)\n",
    "        review = re.sub(r\"\\'ve\", \" have\", review)\n",
    "        review = re.sub(r\"\\'m\", \" am\", review)\n",
    "        return review\n",
    "\n",
    "    def clean(self) -> None:\n",
    "        self.InputDataFrame['review_body'] = self.InputDataFrame['review_body'].apply(lambda text: self.contraction_function(text))\n",
    "\n",
    "class Lowercase():\n",
    "    def __init__(self, InputDataFrame) -> None:\n",
    "        self.InputDataFrame = InputDataFrame\n",
    "        \n",
    "    def clean(self) -> None:\n",
    "        self.InputDataFrame['review_body'] = self.InputDataFrame['review_body'].apply(lambda text: str(text).lower())\n",
    "\n",
    "class RemoveHTMLURL():\n",
    "    def __init__(self, InputDataFrame) -> None:\n",
    "        self.InputDataFrame = InputDataFrame\n",
    "        \n",
    "    def html_url(self, review):\n",
    "        review = re.sub('https?://\\S+|www\\.\\S+', '', review) # html\n",
    "        review = re.sub('<[^<]+?>', '', review)              # url\n",
    "        return review\n",
    "\n",
    "    def clean(self) -> None:\n",
    "        self.InputDataFrame['review_body'] = self.InputDataFrame['review_body'].apply(lambda text: self.html_url(text))\n",
    "\n",
    "class RemoveNonAlphabeticalCharacter():\n",
    "    def __init__(self, InputDataFrame) -> None:\n",
    "        self.InputDataFrame = InputDataFrame\n",
    "        \n",
    "    def clean(self) -> None:\n",
    "        self.InputDataFrame['review_body'] = self.InputDataFrame['review_body'].apply(lambda text: re.sub('[^a-zA-Z\\s]' , '', text))\n",
    "    \n",
    "class RemoveExtraSpaces():\n",
    "    def __init__(self, InputDataFrame) -> None:\n",
    "        self.InputDataFrame = InputDataFrame\n",
    "        \n",
    "    def clean(self) -> None:\n",
    "        self.InputDataFrame['review_body'] = self.InputDataFrame['review_body'].apply(lambda text: re.sub(' +', ' ', text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqW3bt0Hb7Kk"
   },
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1676777688921,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "vh7k88xq5VLb"
   },
   "outputs": [],
   "source": [
    "contraction = Contraction(InputDataFrameHW1)\n",
    "contraction.clean()\n",
    "\n",
    "data_cleaning_lowercase = Lowercase(InputDataFrameHW1)\n",
    "data_cleaning_lowercase.clean()\n",
    "\n",
    "data_cleaning_remove_html_url = RemoveHTMLURL(InputDataFrameHW1)\n",
    "data_cleaning_remove_html_url.clean()\n",
    "\n",
    "remove_non_alphabetical_character = RemoveNonAlphabeticalCharacter(InputDataFrameHW1)\n",
    "remove_non_alphabetical_character.clean()\n",
    "\n",
    "remove_white_space = RemoveExtraSpaces(InputDataFrameHW1)\n",
    "remove_white_space.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "star_rating    0\n",
       "review_body    0\n",
       "class          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputDataFrameHW1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid the computational burden, select 20,000 random reviews from each rating class and create a balanced dataset to perform the required tasks on the downsized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputDataFrame_1 = InputDataFrameHW1.loc[InputDataFrameHW1['class'] == 0].sample(n = 20000)\n",
    "InputDataFrame_2 = InputDataFrameHW1.loc[InputDataFrameHW1['class'] == 1].sample(n = 20000)\n",
    "InputDataFrame_3 = InputDataFrameHW1.loc[InputDataFrameHW1['class'] == 2].sample(n = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20000\n",
       "1    20000\n",
       "2    20000\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reference - https://pandas.pydata.org/docs/user_guide/merging.html\n",
    "\n",
    "InputDataFrames = [InputDataFrame_1, InputDataFrame_2, InputDataFrame_3]\n",
    "InputDataFrameFinalHW1 = pd.concat(InputDataFrames)\n",
    "\n",
    "del InputDataFrame_1, InputDataFrame_2, InputDataFrame_3\n",
    "InputDataFrameFinalHW1['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of reviews before and after data cleaning\n",
      "253.43061308343476, 243.177856800291\n"
     ]
    }
   ],
   "source": [
    "print(\"Average length of reviews before and after data cleaning\")\n",
    "print(average_length_print(before_cleaning, InputDataFrameHW1[\"review_body\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "Using NLTK package to process the dataset by remove the stop words and tokenizing the reviews and applying part of speech tagging technique to identify part of speech for each word in the review for accurate performance of lemmatization <br>\n",
    "\n",
    "#### lemmatization() \n",
    "code to perform lemmatization where each review text is split into list of words and each word is assigned a part of speech tag and based on the part of speech tag each word is lemmatized with the help of WordNetLemmatizer of nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_preprocessing = average_length(InputDataFrameFinalHW1[\"review_body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(review) -> None:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(review):\n",
    "        if (review is None):\n",
    "            return review\n",
    "        else:\n",
    "            if tag.startswith('NN'):\n",
    "                pos = 'n'\n",
    "            elif tag.startswith('VB'):\n",
    "                pos = 'v'\n",
    "            else:\n",
    "                pos = 'a'\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "InputDataFrameFinalHW1['review_body'] = InputDataFrameFinalHW1['review_body'].apply(lambda text: ' '.join(lemmatization(text.split()) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of reviews before and after data preprocessing\n",
      "258.4317, 248.80425\n"
     ]
    }
   ],
   "source": [
    "print(\"Average length of reviews before and after data preprocessing\")\n",
    "print(average_length_print(before_preprocessing, InputDataFrameFinalHW1[\"review_body\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20000\n",
       "1    20000\n",
       "2    20000\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# InputDataFrameFinalHW1.to_csv('Dataset_Final_Data_HW1.csv', index=False)\n",
    "# InputDataFrameFinalHW1 = pd.read_csv('Dataset_Final_Data_HW1.csv')\n",
    "InputDataFrameFinalHW1['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "star_rating    0\n",
       "review_body    0\n",
       "class          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputDataFrameFinalHW1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>962866</th>\n",
       "      <td>1</td>\n",
       "      <td>roll my pressure sock into a painful band half...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2814727</th>\n",
       "      <td>1</td>\n",
       "      <td>buy these to replace another pair of lacross t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163608</th>\n",
       "      <td>1</td>\n",
       "      <td>ridiculous price i purchase the exact same pro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226023</th>\n",
       "      <td>2</td>\n",
       "      <td>i read a lot of review before buy good on fine...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3093646</th>\n",
       "      <td>1</td>\n",
       "      <td>i have a hair that like a lot of moisture and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body  class\n",
       "962866             1  roll my pressure sock into a painful band half...      0\n",
       "2814727            1  buy these to replace another pair of lacross t...      0\n",
       "163608             1  ridiculous price i purchase the exact same pro...      0\n",
       "1226023            2  i read a lot of review before buy good on fine...      0\n",
       "3093646            1  i have a hair that like a lot of moisture and ...      0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputDataFrameFinalHW1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning_remove_html_url = RemoveHTMLURL(InputDataFrameHW3)\n",
    "data_cleaning_remove_html_url.clean()\n",
    "\n",
    "remove_non_alphabetical_character = RemoveNonAlphabeticalCharacter(InputDataFrameHW3)\n",
    "remove_non_alphabetical_character.clean()\n",
    "\n",
    "remove_white_space = RemoveExtraSpaces(InputDataFrameHW3)\n",
    "remove_white_space.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of reviews before and after data cleaning\n",
      "253.43061308343476, 241.223850965477\n"
     ]
    }
   ],
   "source": [
    "print(\"Average length of reviews before and after data cleaning\")\n",
    "print(average_length_print(before_cleaning, InputDataFrameHW3[\"review_body\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "star_rating    0\n",
       "review_body    0\n",
       "class          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputDataFrameHW3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputDataFrameHW3.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputDataFrame_1 = InputDataFrameHW3.loc[InputDataFrameHW3['class'] == 0].sample(n = 20000)\n",
    "InputDataFrame_2 = InputDataFrameHW3.loc[InputDataFrameHW3['class'] == 1].sample(n = 20000)\n",
    "InputDataFrame_3 = InputDataFrameHW3.loc[InputDataFrameHW3['class'] == 2].sample(n = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20000\n",
       "1    20000\n",
       "2    20000\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reference - https://pandas.pydata.org/docs/user_guide/merging.html\n",
    "\n",
    "InputDataFrames = [InputDataFrame_1, InputDataFrame_2, InputDataFrame_3]\n",
    "InputDataFrameFinalHW3 = pd.concat(InputDataFrames)\n",
    "\n",
    "del InputDataFrame_1, InputDataFrame_2, InputDataFrame_3\n",
    "InputDataFrameFinalHW3['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InputDataFrameFinalHW3.to_csv('Dataset_Final_Data_HW3.csv', index=False)\n",
    "# InputDataFrameFinalHW3 = pd.read_csv('Dataset_Final_Data_HW3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "star_rating    0\n",
       "review_body    0\n",
       "class          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputDataFrameFinalHW3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2133250</th>\n",
       "      <td>1</td>\n",
       "      <td>had to return did not include small travel bot...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847125</th>\n",
       "      <td>1</td>\n",
       "      <td>this product have me a rash that lasted for da...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4535979</th>\n",
       "      <td>1</td>\n",
       "      <td>This paper leaves on the skin traces of colore...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347591</th>\n",
       "      <td>2</td>\n",
       "      <td>Just OK stick with the towel version The glove...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4810657</th>\n",
       "      <td>1</td>\n",
       "      <td>I had a this done a year ago I was happy when ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body  class\n",
       "2133250            1  had to return did not include small travel bot...      0\n",
       "847125             1  this product have me a rash that lasted for da...      0\n",
       "4535979            1  This paper leaves on the skin traces of colore...      0\n",
       "2347591            2  Just OK stick with the towel version The glove...      0\n",
       "4810657            1  I had a this done a year ago I was happy when ...      0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputDataFrameFinalHW3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqBC6mLGvSYS"
   },
   "source": [
    "# 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76mF5pKRvvG7"
   },
   "source": [
    "## Part (a)\n",
    "\n",
    "Word2Vec features are extracted using the pretrained dataset generated using the Gensim library and the semantic similarities are check for three example using the pretrained model '__Word2Vec_Pretrained_Model__'<br>\n",
    "Reference - https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 112395,
     "status": "ok",
     "timestamp": 1676788521928,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "_eTfElV7vRnf"
   },
   "outputs": [],
   "source": [
    "Word2Vec_Pretrained_Model = gensim.downloader.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 720,
     "status": "ok",
     "timestamp": 1676779734770,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "eIqgrm-Gvikm",
    "outputId": "e350cca2-2e72-4d24-dbb0-3b585dfa9354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King - Man + Woman = Queen\n"
     ]
    }
   ],
   "source": [
    "# Compute vector arithmetic\n",
    "result1 = Word2Vec_Pretrained_Model.most_similar(positive=['Woman', 'King'], negative=['Man'], topn=1)\n",
    "# result2 = Word2Vec_Pretrained_Model.most_similar(positive=['Madrid', 'France'], negative=['Spain'], topn=1)\n",
    "# result3 = Word2Vec_Pretrained_Model.most_similar(positive=['Actor', 'Female'], negative=['Male'], topn=1)\n",
    "\n",
    "# Display the results\n",
    "print(\"King - Man + Woman =\", result1[0][0])\n",
    "# print(\"Madrid - Spain + France =\", result2[0][0])\n",
    "# print(\"Actor - Male + Female =\", result3[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1676779697702,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "vru3c0he2mkN",
    "outputId": "1ec70af4-3e71-4e2a-9d9c-ed8dfa1750fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great ~ terrific\n",
      "bad ~ good\n"
     ]
    }
   ],
   "source": [
    "result4 = Word2Vec_Pretrained_Model.most_similar('great', topn=1)\n",
    "result5 = Word2Vec_Pretrained_Model.most_similar('bad', topn=1)\n",
    "print(\"great ~\", result4[0][0])\n",
    "print(\"bad ~\", result5[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D8amQHZv186"
   },
   "source": [
    "## Part (b)\n",
    "\n",
    "Word2Vec features are extracted using the dataset generated. The semantic similarities are check for same three example as Part (a) using the trained model '__Word2Vec_Trained_Model__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 165,
     "status": "ok",
     "timestamp": 1676780263768,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "ngo2y8me4f7a",
    "outputId": "500bff7c-0558-490a-d92f-8ad224ca2d1f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2133250</th>\n",
       "      <td>1</td>\n",
       "      <td>had to return did not include small travel bot...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847125</th>\n",
       "      <td>1</td>\n",
       "      <td>this product have me a rash that lasted for da...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4535979</th>\n",
       "      <td>1</td>\n",
       "      <td>This paper leaves on the skin traces of colore...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347591</th>\n",
       "      <td>2</td>\n",
       "      <td>Just OK stick with the towel version The glove...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4810657</th>\n",
       "      <td>1</td>\n",
       "      <td>I had a this done a year ago I was happy when ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body  class\n",
       "2133250            1  had to return did not include small travel bot...      0\n",
       "847125             1  this product have me a rash that lasted for da...      0\n",
       "4535979            1  This paper leaves on the skin traces of colore...      0\n",
       "2347591            2  Just OK stick with the towel version The glove...      0\n",
       "4810657            1  I had a this done a year ago I was happy when ...      0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputDataFrameFinalHW3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty list called review_list to store the review texts is created. The function Loops over the rows of the dataframe '__InputDataFrameFinalHW3__' using the iterrows() function and appends the review_body column of each row to the review_list using the append() function. The review_list should contain all the review texts from the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 2862,
     "status": "ok",
     "timestamp": 1676780465127,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "k6coxnOI5KWn"
   },
   "outputs": [],
   "source": [
    "# Empty list to store the review texts\n",
    "review_list = []\n",
    "\n",
    "# Loop over the rows of the dataframe and append the review texts to the list\n",
    "for index, row in InputDataFrameFinalHW3.iterrows():\n",
    "    review_list.append(row['review_body'])\n",
    "\n",
    "# print(review_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "review_list dataset is loaded as a list of strings and each document in the dataset is tokenized using the gensim.utils.simple_preprocess() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 3358,
     "status": "ok",
     "timestamp": 1676781447590,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "kTZ1Y2lFLhu1"
   },
   "outputs": [],
   "source": [
    "# Load the dataset - review_list\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_data = [gensim.utils.simple_preprocess(doc) for doc in review_list]\n",
    "\n",
    "# print(tokenized_data[45])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec model is trained on the tokenized data using the gensim.models.Word2Vec() function. Here, we set the size parameter to 300 to create 300-dimensional word embeddings, the window parameter to 13 to set the maximum distance between the current and predicted word within a sentence, and the min_count parameter to 9 to only consider words that appear at least nine time in the dataset. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "executionInfo": {
     "elapsed": 25579,
     "status": "ok",
     "timestamp": 1676781556339,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "vQ9j7pvZIgGP"
   },
   "outputs": [],
   "source": [
    "# Train the Word2Vec model\n",
    "Word2Vec_Trained_Model = gensim.models.Word2Vec(tokenized_data, vector_size=300, window=13, min_count=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 157,
     "status": "ok",
     "timestamp": 1676781785468,
     "user": {
      "displayName": "Advait Hemant Naik",
      "userId": "14959022284642815203"
     },
     "user_tz": 480
    },
    "id": "z2IC2H9kMc1j",
    "outputId": "9e6fc95e-c2d0-4099-869e-296e6176dd92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King - Man + Woman = excelent\n"
     ]
    }
   ],
   "source": [
    "# Compute vector arithmetic\n",
    "result1 = Word2Vec_Trained_Model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "# result2 = Word2Vec_Trained_Model.wv.most_similar(positive=['Madrid', 'France'], negative=['Spain'], topn=1)\n",
    "# result3 = Word2Vec_Trained_Model.wv.most_similar(positive=['actor', 'female'], negative=['male'], topn=1)\n",
    "\n",
    "# Display the results\n",
    "print(\"King - Man + Woman =\", result1[0][0])\n",
    "# print(\"Madrid - Spain + France =\", result2[0][0])\n",
    "# print(\"Actor - Male + Female =\", result3[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great ~ good\n",
      "bad ~ terrible\n"
     ]
    }
   ],
   "source": [
    "result4 = Word2Vec_Trained_Model.wv.most_similar('great', topn=1)\n",
    "result5 = Word2Vec_Trained_Model.wv.most_similar('bad', topn=1)\n",
    "print(\"great ~\", result4[0][0])\n",
    "print(\"bad ~\", result5[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude from comparing vectors generated by yourself and the pretrained model?\n",
    "\n",
    "The Word2Vec_Pretrained_Model is pre-trained on a large dataset and should have a more generalized understanding of word relationships, whereas the Word2Vec_Trained_Model is trained on a specific dataset and may have a more specialized understanding of word relationships specific to that dataset.\n",
    "\n",
    "### Which of the Word2Vec models seems to encode semantic similarities between words better?\n",
    "\n",
    "Word2Vec_Pretrained_Model seems to encode semantic similarities better, as it produces the well-known semantic analogy: King - Man + Woman = Queen. In contrast, the Word2Vec_Trained_Model produces a less coherent result for the same analogy, i.e., King - Man + Woman = excellent, which doesn't make sense semantically.\n",
    "\n",
    "Furthermore, based on the given information, the Word2Vec_Pretrained_Model also seems to encode semantic similarities better for the word pairs \"great ~ terrific\" and \"bad ~ good,\" as it produces the expected relationships, whereas the Word2Vec_Trained_Model produces the opposite relationship for \"bad ~ good,\" which is not ideal. Therefore, the Word2Vec_Pretrained_Model seems to be a better model for encoding semantic similarities between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKiOSTweLk2u"
   },
   "source": [
    "# 3. Simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load the data into a pandas dataframe\n",
    "# - InputDataFrameFinalHW1\n",
    "# - InputDataFrameFinalHW3\n",
    "\n",
    "# Load the pre-trained Word2Vec model from Google News dataset \n",
    "# - Word2Vec_Pretrained_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYPckJUSiajX"
   },
   "source": [
    "### TF-IDF Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF = TfidfVectorizer(ngram_range=(1,4))\n",
    "X_tf = TF_IDF.fit_transform(InputDataFrameFinalHW1[\"review_body\"])\n",
    "Y_tf = InputDataFrameFinalHW1['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf ,x_test_tf, y_train_tf, y_test_tf = train_test_split(X_tf, Y_tf, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### averageWord2VecVector()\n",
    "\n",
    "The average Word2Vec vectors for each review as the input feature (x = $ \\frac{1}{N} \\sum_{i=1} ^{N} W_{i} $ for a review with N words) is calculated using the averageWord2VecVector() function <br>\n",
    "\n",
    "The loop iterates through each word in the review, splits the review string into individual words, and checks if each word is present in the pre-trained __Word2Vec_Pretrained_Model__ model. If the word is present, its corresponding vector is added to the vectors list. <br>\n",
    "\n",
    "If there are no words in the review that are present in the pre-trained Word2Vec_Pretrained_Model model, then the function returns a zero vector with the same dimensionality as the Word2Vec_Pretrained_Model vectors (word2vec_dim=300). <br>\n",
    "\n",
    "Finally, the function returns the average vector of all the vectors in the vectors list. np.mean calculates the mean of all the vectors along the first axis (which corresponds to the individual dimensions of the vectors), resulting in a single 300-dimensional vector representing the review's average Word2Vec vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "MyZplWqAWhvW"
   },
   "outputs": [],
   "source": [
    "word2vec_dim=300\n",
    "\n",
    "def averageWord2VecVector(review):\n",
    "    vectors = []\n",
    "    for word in review.split():\n",
    "        if word in Word2Vec_Pretrained_Model:\n",
    "            vectors.append(Word2Vec_Pretrained_Model[word])\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(word2vec_dim)\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average Word2Vec vectors for all reviews in the dataset\n",
    "X = np.vstack(InputDataFrameFinalHW3['review_body'].apply(lambda x: averageWord2VecVector(x)))\n",
    "\n",
    "# Define the target variable\n",
    "Y = InputDataFrameFinalHW3['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 300)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets random_state=42\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron (TF-IDF Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron accuracy (TF-IDF Feature): 0.72275\n"
     ]
    }
   ],
   "source": [
    "model_perceptron = Perceptron()\n",
    "model_perceptron.fit(x_train_tf,y_train_tf)\n",
    "y_pred_tf = model_perceptron.predict(x_test_tf)\n",
    "print(\"Perceptron accuracy (TF-IDF Feature):\", accuracy_score(y_test_tf,y_pred_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron (Word2Vec Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron accuracy (Word2Vec Feature): 0.54825\n"
     ]
    }
   ],
   "source": [
    "model_perceptron = Perceptron()\n",
    "model_perceptron.fit(x_train,y_train)\n",
    "y_pred = model_perceptron.predict(x_test)\n",
    "print(\"Perceptron accuracy (Word2Vec Feature):\", accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM (TF-IDF Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy (TF-IDF Feature): 0.7465833333333334\n"
     ]
    }
   ],
   "source": [
    "model_svm = LinearSVC()\n",
    "model_svm.fit(x_train_tf,y_train_tf)\n",
    "y_pred_tf = model_svm.predict(x_test_tf)\n",
    "print(\"SVM accuracy (TF-IDF Feature):\", accuracy_score(y_test_tf,y_pred_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM (Word2Vec Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy (Word2Vec Feature): 0.6455833333333333\n"
     ]
    }
   ],
   "source": [
    "model_svm = LinearSVC()\n",
    "model_svm.fit(x_train,y_train)\n",
    "y_pred = model_svm.predict(x_test)\n",
    "print(\"SVM accuracy (Word2Vec Feature):\", accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained Word2Vec features)?\n",
    "\n",
    "The models trained using the TF-IDF feature outperform the models trained using the pretrained Word2Vec feature. The SVM classifier achieves an accuracy of 0.7465834 using the TF-IDF feature, which is about 10% higher than the accuracy achieved by the same classifier using the Word2Vec feature (0.645583). Similarly, the Perceptron classifier achieves an accuracy of 0.72275 using the TF-IDF feature, which is about 17% higher than the accuracy achieved by the same classifier using the Word2Vec feature (0.54825).\n",
    "\n",
    "Hence, the TF-IDF feature is more effective than the pretrained Word2Vec feature in representing the sentiment of the amazon product reviews. It's important to note that the effectiveness of a feature representation can vary depending on the task and dataset, and it's possible that the Word2Vec feature may perform better on other sentiment analysis tasks or datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PG2XZhJL9VT"
   },
   "source": [
    "# 4. Feedforward Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load the data into a pandas dataframe\n",
    "# - InputDataFrameFinalHW3\n",
    "\n",
    "# Load the pre-trained Word2Vec model from Google News dataset \n",
    "# - Word2Vec_Pretrained_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward Multilayer Perceptron Network for classification is trained using Word2Vec_Pretrained_Model. The network with two hidden layers, each with 100 and 10 nodes, respectively is constructed. <br>\n",
    "\n",
    "#### MLP(nn.Module)\n",
    "\n",
    "The __init__ method is called when an instance of the MLP class is created. It initializes the model's layers, activation functions, and other parameters. Here, the method takes four arguments: <br>\n",
    "\n",
    "__input_size__ - The size of the input vector for the model - 300 <br>\n",
    "__hidden_layer_1_size__ - The number of nodes in the first hidden layer - 100 <br>\n",
    "__hidden_layer_2_size__ - The number of nodes in the second hidden layer - 10 <br>\n",
    "__output_size__ The size of the output vector for the model - 3 <br>\n",
    "The super() function is used to call the constructor of the parent class nn.Module, which initializes the module. <br>\n",
    "\n",
    "The fully connected layers (also known as linear layers) of the MLP model. nn.Linear is a PyTorch module that represents a linear transformation of the input data. Each nn.Linear object is assigned to an instance variable in the MLP class, self.fc1, self.fc2, and self.fc3, respectively. <br>\n",
    "\n",
    "The activation functions for the model. nn.ReLU() creates an instance of the rectified linear unit (ReLU) activation function, which applies the element-wise rectified linear function to the input data. nn.Softmax(dim=1) creates an instance of the softmax activation function, which applies a normalized exponential function to the input data to produce a probability distribution over the output classes. <br>\n",
    "\n",
    "####  forward(self, x) \n",
    "\n",
    "This method defines how the input data x flows through the layers of the model. The forward method takes the input tensor x as its argument, and applies the fully connected layers and activation functions to it. The output tensor is then returned by the method. Specifically, the input tensor x is passed through the first hidden layer (self.fc1) and the ReLU activation function (self.relu), then through the second hidden layer (self.fc2) and the ReLU activation function, and finally through the output layer (self.fc3) and the softmax activation function (self.softmax). The resulting tensor is returned by the method. <br>\n",
    "\n",
    "Reference - https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "# This defines a PyTorch Module called MLP, which will represent multi-layer perceptron (MLP) model.\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_1_size, hidden_layer_2_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_layer_1_size)\n",
    "        self.fc2 = nn.Linear(hidden_layer_1_size, hidden_layer_2_size)\n",
    "        self.fc3 = nn.Linear(hidden_layer_2_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Dataset class for data\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.FloatTensor(self.X[index]), torch.FloatTensor(self.Y[index])\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a) - Average\n",
    "\n",
    "The input features are generated, use the average Word2Vec vectors similar to the '__Simple models__' section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_dim=300\n",
    "\n",
    "def averageWord2VecVector(review):\n",
    "    vectors = []\n",
    "    for word in review.split():\n",
    "        if word in Word2Vec_Pretrained_Model:\n",
    "            vectors.append(Word2Vec_Pretrained_Model[word])\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(word2vec_dim)\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average Word2Vec vectors for all reviews in the dataset\n",
    "X = np.vstack(InputDataFrameFinalHW3['review_body'].apply(lambda x: averageWord2VecVector(x)))\n",
    "\n",
    "# Define the target variable\n",
    "Y = InputDataFrameFinalHW3['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 300)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding\n",
    "Y_tensor = torch.tensor(Y)\n",
    "Y_onehot = torch.nn.functional.one_hot(Y_tensor)\n",
    "Y= Y_onehot.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "input_size = 300 # 300\n",
    "hidden_layer_1_size = 100\n",
    "hidden_layer_2_size = 10\n",
    "output_size = 3\n",
    "learning_rate = 0.001 \n",
    "num_workers = 6\n",
    "batch_size = 64\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = MLP(input_size, hidden_layer_1_size, hidden_layer_2_size, output_size)\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the Dataset class for the training and testing data\n",
    "# Create DataLoader objects for the training and testing data\n",
    "\n",
    "training_set = Dataset(x_train, y_train)\n",
    "training_generator = data.DataLoader(training_set, batch_size = batch_size, shuffle = True)\n",
    "testing_set = Dataset(x_test, y_test)\n",
    "testing_generator = data.DataLoader(testing_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t Training Loss: 0.969062\n",
      "Epoch 2 \t Training Loss: 0.904044\n",
      "Epoch 3 \t Training Loss: 0.893738\n",
      "Epoch 4 \t Training Loss: 0.888281\n",
      "Epoch 5 \t Training Loss: 0.886398\n",
      "Epoch 6 \t Training Loss: 0.883098\n",
      "Epoch 7 \t Training Loss: 0.881297\n",
      "Epoch 8 \t Training Loss: 0.879152\n",
      "Epoch 9 \t Training Loss: 0.877107\n",
      "Epoch 10 \t Training Loss: 0.875518\n",
      "Epoch 11 \t Training Loss: 0.874424\n",
      "Epoch 12 \t Training Loss: 0.872270\n",
      "Epoch 13 \t Training Loss: 0.871687\n",
      "Epoch 14 \t Training Loss: 0.869544\n",
      "Epoch 15 \t Training Loss: 0.867678\n",
      "Epoch 16 \t Training Loss: 0.866549\n",
      "Epoch 17 \t Training Loss: 0.864641\n",
      "Epoch 18 \t Training Loss: 0.864032\n",
      "Epoch 19 \t Training Loss: 0.861388\n",
      "Epoch 20 \t Training Loss: 0.859432\n",
      "Epoch 21 \t Training Loss: 0.858595\n",
      "Epoch 22 \t Training Loss: 0.857860\n",
      "Epoch 23 \t Training Loss: 0.855605\n",
      "Epoch 24 \t Training Loss: 0.854491\n",
      "Epoch 25 \t Training Loss: 0.851912\n",
      "Epoch 26 \t Training Loss: 0.851597\n",
      "Epoch 27 \t Training Loss: 0.849830\n",
      "Epoch 28 \t Training Loss: 0.847800\n",
      "Epoch 29 \t Training Loss: 0.846442\n",
      "Epoch 30 \t Training Loss: 0.844867\n",
      "Epoch 31 \t Training Loss: 0.843248\n",
      "Epoch 32 \t Training Loss: 0.841522\n",
      "Epoch 33 \t Training Loss: 0.839250\n",
      "Epoch 34 \t Training Loss: 0.837589\n",
      "Epoch 35 \t Training Loss: 0.835953\n",
      "Epoch 36 \t Training Loss: 0.834678\n",
      "Epoch 37 \t Training Loss: 0.832780\n",
      "Epoch 38 \t Training Loss: 0.830681\n",
      "Epoch 39 \t Training Loss: 0.828928\n",
      "Epoch 40 \t Training Loss: 0.827160\n",
      "Epoch 41 \t Training Loss: 0.825529\n",
      "Epoch 42 \t Training Loss: 0.824154\n",
      "Epoch 43 \t Training Loss: 0.821762\n",
      "Epoch 44 \t Training Loss: 0.819895\n",
      "Epoch 45 \t Training Loss: 0.818884\n",
      "Epoch 46 \t Training Loss: 0.816695\n",
      "Epoch 47 \t Training Loss: 0.814809\n",
      "Epoch 48 \t Training Loss: 0.813742\n",
      "Epoch 49 \t Training Loss: 0.813971\n",
      "Epoch 50 \t Training Loss: 0.810880\n",
      "Epoch 51 \t Training Loss: 0.808421\n",
      "Epoch 52 \t Training Loss: 0.807973\n",
      "Epoch 53 \t Training Loss: 0.806545\n",
      "Epoch 54 \t Training Loss: 0.804237\n",
      "Epoch 55 \t Training Loss: 0.803231\n",
      "Epoch 56 \t Training Loss: 0.802416\n",
      "Epoch 57 \t Training Loss: 0.800652\n",
      "Epoch 58 \t Training Loss: 0.799986\n",
      "Epoch 59 \t Training Loss: 0.796794\n",
      "Epoch 60 \t Training Loss: 0.796971\n",
      "Epoch 61 \t Training Loss: 0.794583\n",
      "Epoch 62 \t Training Loss: 0.792888\n",
      "Epoch 63 \t Training Loss: 0.792471\n",
      "Epoch 64 \t Training Loss: 0.790828\n",
      "Epoch 65 \t Training Loss: 0.791018\n",
      "Epoch 66 \t Training Loss: 0.788433\n",
      "Epoch 67 \t Training Loss: 0.787657\n",
      "Epoch 68 \t Training Loss: 0.785272\n",
      "Epoch 69 \t Training Loss: 0.784601\n",
      "Epoch 70 \t Training Loss: 0.783962\n",
      "Epoch 71 \t Training Loss: 0.781805\n",
      "Epoch 72 \t Training Loss: 0.782032\n",
      "Epoch 73 \t Training Loss: 0.779557\n",
      "Epoch 74 \t Training Loss: 0.780551\n",
      "Epoch 75 \t Training Loss: 0.777851\n",
      "Epoch 76 \t Training Loss: 0.776338\n",
      "Epoch 77 \t Training Loss: 0.776182\n",
      "Epoch 78 \t Training Loss: 0.776348\n",
      "Epoch 79 \t Training Loss: 0.774145\n",
      "Epoch 80 \t Training Loss: 0.772683\n",
      "Epoch 81 \t Training Loss: 0.771658\n",
      "Epoch 82 \t Training Loss: 0.770589\n",
      "Epoch 83 \t Training Loss: 0.770590\n",
      "Epoch 84 \t Training Loss: 0.769045\n",
      "Epoch 85 \t Training Loss: 0.770361\n",
      "Epoch 86 \t Training Loss: 0.769410\n",
      "Epoch 87 \t Training Loss: 0.766801\n",
      "Epoch 88 \t Training Loss: 0.766122\n",
      "Epoch 89 \t Training Loss: 0.765456\n",
      "Epoch 90 \t Training Loss: 0.764921\n",
      "Epoch 91 \t Training Loss: 0.764369\n",
      "Epoch 92 \t Training Loss: 0.763091\n",
      "Epoch 93 \t Training Loss: 0.763989\n",
      "Epoch 94 \t Training Loss: 0.762474\n",
      "Epoch 95 \t Training Loss: 0.761444\n",
      "Epoch 96 \t Training Loss: 0.761521\n",
      "Epoch 97 \t Training Loss: 0.759038\n",
      "Epoch 98 \t Training Loss: 0.759968\n",
      "Epoch 99 \t Training Loss: 0.759930\n",
      "Epoch 100 \t Training Loss: 0.758555\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in training_generator:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch %d \\t Training Loss: %.6f' % (epoch + 1, running_loss/len(training_generator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Evaluate(model, testing_generator):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testing_generator:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    print('Accuracy Feedforward Neural Networks (Average): %.6f' % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Feedforward Neural Networks (Average): 0.660667\n"
     ]
    }
   ],
   "source": [
    "Model_Evaluate(model, testing_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b) - Concatenate\n",
    "\n",
    "The input features are generated by concatenating the first 10 Word2Vec vectors for each review as the input feature $( x = [W_{T} ^{1}, ..., W_{T} ^{10}])$ and train the neural network.\n",
    "\n",
    "#### concatenateWord2VecVector(review)\n",
    "\n",
    "This function takes a review and the Word2Vec model as inputs and returns the input feature for the review. The function first splits the review into words and checks if each word is in the __Word2Vec_Pretrained_Model__ model. If a word is in the model, the corresponding word vector is added to a vectors list. If the list has less than 10 vectors, it is padded with zero vectors. Finally, the first 10 vectors are concatenated into a single input feature vector.\n",
    "\n",
    "The input features are constructed for all reviews in the dataset by applying the concatenateWord2VecVector function to each review in the 'review_body' column of the dataframe. The resulting input features are stacked vertically into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate the input feature for a review\n",
    "word2vec_dim = 300\n",
    "review_length = 10\n",
    "\n",
    "def concatenateWord2VecVector(review):\n",
    "    vectors = []\n",
    "    for word in review.split():\n",
    "        if word in Word2Vec_Pretrained_Model:\n",
    "            vectors.append(Word2Vec_Pretrained_Model[word])\n",
    "    if len(vectors) < review_length:\n",
    "        vectors += [np.zeros(word2vec_dim)] * (review_length - len(vectors))\n",
    "    return np.concatenate(vectors[:review_length], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the input features for all reviews in the dataset\n",
    "X = np.vstack(InputDataFrameFinalHW3['review_body'].apply(lambda x: concatenateWord2VecVector(x)))\n",
    "\n",
    "# Define the target variable\n",
    "Y = InputDataFrameFinalHW3['class'].values\n",
    "\n",
    "# Perform one-hot encoding\n",
    "Y_tensor = torch.tensor(Y)\n",
    "Y_onehot = torch.nn.functional.one_hot(Y_tensor)\n",
    "Y = Y_onehot.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 3000)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 3)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "input_size = 3000 # 300 * 10\n",
    "hidden_layer_1_size = 100\n",
    "hidden_layer_2_size = 10\n",
    "output_size = 3\n",
    "learning_rate = 0.001 \n",
    "batch_size = 64\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the Dataset class for the training and testing data\n",
    "# Create DataLoader objects for the training and testing data\n",
    "\n",
    "training_set = Dataset(x_train, y_train)\n",
    "training_generator = data.DataLoader(training_set, batch_size = batch_size, shuffle = True)\n",
    "testing_set = Dataset(x_test, y_test)\n",
    "testing_generator = data.DataLoader(testing_set, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=3000, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = MLP(input_size, hidden_layer_1_size, hidden_layer_2_size, output_size)\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t Training Loss: 0.991617\n",
      "Epoch 2 \t Training Loss: 0.938220\n",
      "Epoch 3 \t Training Loss: 0.913522\n",
      "Epoch 4 \t Training Loss: 0.889798\n",
      "Epoch 5 \t Training Loss: 0.861253\n",
      "Epoch 6 \t Training Loss: 0.831912\n",
      "Epoch 7 \t Training Loss: 0.805143\n",
      "Epoch 8 \t Training Loss: 0.781526\n",
      "Epoch 9 \t Training Loss: 0.763044\n",
      "Epoch 10 \t Training Loss: 0.750060\n",
      "Epoch 11 \t Training Loss: 0.739314\n",
      "Epoch 12 \t Training Loss: 0.730753\n",
      "Epoch 13 \t Training Loss: 0.725482\n",
      "Epoch 14 \t Training Loss: 0.722934\n",
      "Epoch 15 \t Training Loss: 0.717653\n",
      "Epoch 16 \t Training Loss: 0.714221\n",
      "Epoch 17 \t Training Loss: 0.712332\n",
      "Epoch 18 \t Training Loss: 0.709636\n",
      "Epoch 19 \t Training Loss: 0.709847\n",
      "Epoch 20 \t Training Loss: 0.706351\n",
      "Epoch 21 \t Training Loss: 0.703935\n",
      "Epoch 22 \t Training Loss: 0.701922\n",
      "Epoch 23 \t Training Loss: 0.700064\n",
      "Epoch 24 \t Training Loss: 0.700092\n",
      "Epoch 25 \t Training Loss: 0.698028\n",
      "Epoch 26 \t Training Loss: 0.697225\n",
      "Epoch 27 \t Training Loss: 0.696344\n",
      "Epoch 28 \t Training Loss: 0.695427\n",
      "Epoch 29 \t Training Loss: 0.694209\n",
      "Epoch 30 \t Training Loss: 0.693827\n",
      "Epoch 31 \t Training Loss: 0.691924\n",
      "Epoch 32 \t Training Loss: 0.690885\n",
      "Epoch 33 \t Training Loss: 0.690948\n",
      "Epoch 34 \t Training Loss: 0.690324\n",
      "Epoch 35 \t Training Loss: 0.690382\n",
      "Epoch 36 \t Training Loss: 0.689833\n",
      "Epoch 37 \t Training Loss: 0.687816\n",
      "Epoch 38 \t Training Loss: 0.687214\n",
      "Epoch 39 \t Training Loss: 0.687058\n",
      "Epoch 40 \t Training Loss: 0.686728\n",
      "Epoch 41 \t Training Loss: 0.685815\n",
      "Epoch 42 \t Training Loss: 0.684504\n",
      "Epoch 43 \t Training Loss: 0.684581\n",
      "Epoch 44 \t Training Loss: 0.685118\n",
      "Epoch 45 \t Training Loss: 0.684330\n",
      "Epoch 46 \t Training Loss: 0.682759\n",
      "Epoch 47 \t Training Loss: 0.684158\n",
      "Epoch 48 \t Training Loss: 0.682885\n",
      "Epoch 49 \t Training Loss: 0.683139\n",
      "Epoch 50 \t Training Loss: 0.681542\n",
      "Epoch 51 \t Training Loss: 0.681358\n",
      "Epoch 52 \t Training Loss: 0.680023\n",
      "Epoch 53 \t Training Loss: 0.680851\n",
      "Epoch 54 \t Training Loss: 0.680704\n",
      "Epoch 55 \t Training Loss: 0.679377\n",
      "Epoch 56 \t Training Loss: 0.679753\n",
      "Epoch 57 \t Training Loss: 0.679794\n",
      "Epoch 58 \t Training Loss: 0.678314\n",
      "Epoch 59 \t Training Loss: 0.678333\n",
      "Epoch 60 \t Training Loss: 0.678940\n",
      "Epoch 61 \t Training Loss: 0.677255\n",
      "Epoch 62 \t Training Loss: 0.676968\n",
      "Epoch 63 \t Training Loss: 0.676314\n",
      "Epoch 64 \t Training Loss: 0.676018\n",
      "Epoch 65 \t Training Loss: 0.676588\n",
      "Epoch 66 \t Training Loss: 0.677185\n",
      "Epoch 67 \t Training Loss: 0.678946\n",
      "Epoch 68 \t Training Loss: 0.675874\n",
      "Epoch 69 \t Training Loss: 0.675460\n",
      "Epoch 70 \t Training Loss: 0.676012\n",
      "Epoch 71 \t Training Loss: 0.674379\n",
      "Epoch 72 \t Training Loss: 0.675307\n",
      "Epoch 73 \t Training Loss: 0.675314\n",
      "Epoch 74 \t Training Loss: 0.673304\n",
      "Epoch 75 \t Training Loss: 0.672854\n",
      "Epoch 76 \t Training Loss: 0.674813\n",
      "Epoch 77 \t Training Loss: 0.673954\n",
      "Epoch 78 \t Training Loss: 0.673098\n",
      "Epoch 79 \t Training Loss: 0.673512\n",
      "Epoch 80 \t Training Loss: 0.672227\n",
      "Epoch 81 \t Training Loss: 0.673151\n",
      "Epoch 82 \t Training Loss: 0.671578\n",
      "Epoch 83 \t Training Loss: 0.672107\n",
      "Epoch 84 \t Training Loss: 0.672461\n",
      "Epoch 85 \t Training Loss: 0.670870\n",
      "Epoch 86 \t Training Loss: 0.671205\n",
      "Epoch 87 \t Training Loss: 0.671387\n",
      "Epoch 88 \t Training Loss: 0.671024\n",
      "Epoch 89 \t Training Loss: 0.672044\n",
      "Epoch 90 \t Training Loss: 0.670935\n",
      "Epoch 91 \t Training Loss: 0.670806\n",
      "Epoch 92 \t Training Loss: 0.669426\n",
      "Epoch 93 \t Training Loss: 0.669923\n",
      "Epoch 94 \t Training Loss: 0.669113\n",
      "Epoch 95 \t Training Loss: 0.669808\n",
      "Epoch 96 \t Training Loss: 0.668948\n",
      "Epoch 97 \t Training Loss: 0.669379\n",
      "Epoch 98 \t Training Loss: 0.669464\n",
      "Epoch 99 \t Training Loss: 0.669266\n",
      "Epoch 100 \t Training Loss: 0.668856\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in training_generator:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch %d \\t Training Loss: %.6f' % (epoch + 1, running_loss/len(training_generator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Evaluate(model, testing_generator):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testing_generator:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Accuracy Accuracy Feedforward Neural Networks (Concatenate): %.6f' % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Accuracy Feedforward Neural Networks (Concatenate): 0.562333\n"
     ]
    }
   ],
   "source": [
    "Model_Evaluate(model, testing_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do you conclude by comparing accuracy values you obtain with those obtained in the Simple Models section ?\n",
    "\n",
    "The Feedforward Neural Networks (FNN) model using the word embeddings outperforms the \"Simple Models\" (Perceptron and SVM) using the same word embeddings in terms of accuracy. The Perceptron model achieves an accuracy of 0.54825, the SVM model achieves an accuracy of 0.6455834, whereas the Feedforward Neural Networks model using the averaging method achieves an accuracy of 0.660667. The FNN model is more effective than the Perceptron and SVM models in representing the sentiment of the reviews in this particular task and dataset.\n",
    "\n",
    "The two different methods for combining word embeddings are being compared: averaging and concatenating. The accuracy of the Feedforward Neural Networks (FNN) model is reported for both methods, and the accuracy of the model using the averaging method (0.660667) is higher than the accuracy of the model using the concatenation method (0.562333).\n",
    "\n",
    "The averaging method is more effective than the concatenation method in representing the sentiment of the product reviews in this particular task and dataset. The difference in accuracy between the two methods could be due to the fact that the averaging method captures the overall sentiment of the sentence by taking the mean of the word embeddings, whereas the concatenation method concatenates the word embeddings into a longer vector, which may not capture the overall sentiment as effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load the data into a pandas dataframe\n",
    "# - InputDataFrameFinalHW3\n",
    "\n",
    "# Load the pre-trained Word2Vec model from Google News dataset \n",
    "# - Word2Vec_Pretrained_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Network (RNN) using __Word2Vec_Pretrained_Model__ features for classification. For Training a simple RNN an RNN cell with the hidden state size of 20 is considered. To feed your data into our RNN, review length is limited the maximum 20 by truncating longer reviews and padding shorter reviews with a null value (0). To perform the preprocessing rnnWord2VecVector is created<br>\n",
    "Reference - https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html \n",
    "\n",
    "#### rnnWord2VecVector(review)\n",
    "\n",
    "The loop iterates through each word in the review, splits the review string into individual words, and checks if each word is present in the pre-trained Word2Vec_Pretrained_Model model. If the word is present, its corresponding vector is added to the vectors list.<br>\n",
    "If the length of the vectors list is greater than review_length (20), the function truncates it to review_length by keeping only the first review_length vectors.<br>\n",
    "If the length of the vectors list is less than review_length, the function pads it with zero vectors of the same dimensionality as the Word2Vec vectors to make its length equal to review_length.<br>\n",
    "Finally, the function returns an array of vectors representing the review. The length of the array is review_length, and each element is a Word2Vec vector corresponding to a word in the review (or a zero vector if the word is not present in the pre-trained Word2Vec_Pretrained_Model model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate the input feature for a review\n",
    "\n",
    "word2vec_dim = 300\n",
    "review_length = 20\n",
    "\n",
    "def rnnWord2VecVector(review):\n",
    "    vectors = []\n",
    "    for word in review.split():\n",
    "        if word in Word2Vec_Pretrained_Model:\n",
    "            vectors.append(Word2Vec_Pretrained_Model[word])\n",
    "    if len(vectors) > review_length:\n",
    "        vectors = vectors[:review_length]\n",
    "    elif len(vectors) < review_length:\n",
    "        vectors += [np.zeros(word2vec_dim)] * (review_length - len(vectors))\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input features for all reviews in the dataset\n",
    "X = np.vstack(InputDataFrameFinalHW3['review_body'].apply(lambda x: rnnWord2VecVector(x)))\n",
    "X = X.reshape((len(InputDataFrameFinalHW3), review_length, word2vec_dim))\n",
    "\n",
    "# Define the target variable\n",
    "Y = InputDataFrameFinalHW3['class'].values\n",
    "\n",
    "# Perform one-hot encoding\n",
    "Y_tensor = torch.tensor(Y)\n",
    "Y_onehot = torch.nn.functional.one_hot(Y_tensor)\n",
    "Y = Y_onehot.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 20, 300)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 3)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Dataset class for our data\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.FloatTensor(self.X[index]), torch.FloatTensor(self.Y[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the Dataset class for the training and testing data\n",
    "# Create DataLoader objects for the training and testing data\n",
    "batch_size = 64\n",
    "\n",
    "training_set = Dataset(x_train, y_train)\n",
    "training_generator = data.DataLoader(training_set, batch_size = batch_size, shuffle = True)\n",
    "testing_set = Dataset(x_test, y_test)\n",
    "testing_generator = data.DataLoader(testing_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a) - Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the recurrent neural network architecture\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        # Forward propagate RNN\n",
    "        output, _ = self.rnn(x, h0)\n",
    "        # Decode the hidden state of the last time step\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "input_size = 300 # 300 * 20\n",
    "hidden_size = 20\n",
    "num_layers = 1\n",
    "output_size = 3\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(300, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = RNN(input_size, hidden_size, num_layers, output_size)\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t Training Loss: 1.012442\n",
      "Epoch 2 \t Training Loss: 0.947200\n",
      "Epoch 3 \t Training Loss: 0.896272\n",
      "Epoch 4 \t Training Loss: 0.869501\n",
      "Epoch 5 \t Training Loss: 0.854658\n",
      "Epoch 6 \t Training Loss: 0.847672\n",
      "Epoch 7 \t Training Loss: 0.841372\n",
      "Epoch 8 \t Training Loss: 0.833769\n",
      "Epoch 9 \t Training Loss: 0.829456\n",
      "Epoch 10 \t Training Loss: 0.824625\n",
      "Epoch 11 \t Training Loss: 0.821032\n",
      "Epoch 12 \t Training Loss: 0.816276\n",
      "Epoch 13 \t Training Loss: 0.810729\n",
      "Epoch 14 \t Training Loss: 0.809639\n",
      "Epoch 15 \t Training Loss: 0.807310\n",
      "Epoch 16 \t Training Loss: 0.807869\n",
      "Epoch 17 \t Training Loss: 0.800729\n",
      "Epoch 18 \t Training Loss: 0.798637\n",
      "Epoch 19 \t Training Loss: 0.794758\n",
      "Epoch 20 \t Training Loss: 0.795091\n",
      "Epoch 21 \t Training Loss: 0.789704\n",
      "Epoch 22 \t Training Loss: 0.785946\n",
      "Epoch 23 \t Training Loss: 0.786505\n",
      "Epoch 24 \t Training Loss: 0.782175\n",
      "Epoch 25 \t Training Loss: 0.782911\n",
      "Epoch 26 \t Training Loss: 0.777744\n",
      "Epoch 27 \t Training Loss: 0.775959\n",
      "Epoch 28 \t Training Loss: 0.775452\n",
      "Epoch 29 \t Training Loss: 0.773066\n",
      "Epoch 30 \t Training Loss: 0.769983\n",
      "Epoch 31 \t Training Loss: 0.769847\n",
      "Epoch 32 \t Training Loss: 0.769126\n",
      "Epoch 33 \t Training Loss: 0.765537\n",
      "Epoch 34 \t Training Loss: 0.761262\n",
      "Epoch 35 \t Training Loss: 0.763123\n",
      "Epoch 36 \t Training Loss: 0.760006\n",
      "Epoch 37 \t Training Loss: 0.758837\n",
      "Epoch 38 \t Training Loss: 0.755128\n",
      "Epoch 39 \t Training Loss: 0.755949\n",
      "Epoch 40 \t Training Loss: 0.752199\n",
      "Epoch 41 \t Training Loss: 0.748793\n",
      "Epoch 42 \t Training Loss: 0.747613\n",
      "Epoch 43 \t Training Loss: 0.750846\n",
      "Epoch 44 \t Training Loss: 0.745376\n",
      "Epoch 45 \t Training Loss: 0.746813\n",
      "Epoch 46 \t Training Loss: 0.741758\n",
      "Epoch 47 \t Training Loss: 0.746276\n",
      "Epoch 48 \t Training Loss: 0.742908\n",
      "Epoch 49 \t Training Loss: 0.741629\n",
      "Epoch 50 \t Training Loss: 0.748495\n",
      "Epoch 51 \t Training Loss: 0.737899\n",
      "Epoch 52 \t Training Loss: 0.738889\n",
      "Epoch 53 \t Training Loss: 0.739005\n",
      "Epoch 54 \t Training Loss: 0.738511\n",
      "Epoch 55 \t Training Loss: 0.735105\n",
      "Epoch 56 \t Training Loss: 0.733150\n",
      "Epoch 57 \t Training Loss: 0.732751\n",
      "Epoch 58 \t Training Loss: 0.734759\n",
      "Epoch 59 \t Training Loss: 0.733019\n",
      "Epoch 60 \t Training Loss: 0.730641\n",
      "Epoch 61 \t Training Loss: 0.727878\n",
      "Epoch 62 \t Training Loss: 0.727334\n",
      "Epoch 63 \t Training Loss: 0.726205\n",
      "Epoch 64 \t Training Loss: 0.730510\n",
      "Epoch 65 \t Training Loss: 0.725871\n",
      "Epoch 66 \t Training Loss: 0.724586\n",
      "Epoch 67 \t Training Loss: 0.725887\n",
      "Epoch 68 \t Training Loss: 0.721822\n",
      "Epoch 69 \t Training Loss: 0.716466\n",
      "Epoch 70 \t Training Loss: 0.719863\n",
      "Epoch 71 \t Training Loss: 0.719421\n",
      "Epoch 72 \t Training Loss: 0.721713\n",
      "Epoch 73 \t Training Loss: 0.719197\n",
      "Epoch 74 \t Training Loss: 0.715349\n",
      "Epoch 75 \t Training Loss: 0.715734\n",
      "Epoch 76 \t Training Loss: 0.714410\n",
      "Epoch 77 \t Training Loss: 0.716169\n",
      "Epoch 78 \t Training Loss: 0.711369\n",
      "Epoch 79 \t Training Loss: 0.710989\n",
      "Epoch 80 \t Training Loss: 0.710635\n",
      "Epoch 81 \t Training Loss: 0.715226\n",
      "Epoch 82 \t Training Loss: 0.709205\n",
      "Epoch 83 \t Training Loss: 0.709120\n",
      "Epoch 84 \t Training Loss: 0.712725\n",
      "Epoch 85 \t Training Loss: 0.711025\n",
      "Epoch 86 \t Training Loss: 0.706673\n",
      "Epoch 87 \t Training Loss: 0.705504\n",
      "Epoch 88 \t Training Loss: 0.706507\n",
      "Epoch 89 \t Training Loss: 0.707053\n",
      "Epoch 90 \t Training Loss: 0.711752\n",
      "Epoch 91 \t Training Loss: 0.706707\n",
      "Epoch 92 \t Training Loss: 0.703764\n",
      "Epoch 93 \t Training Loss: 0.702694\n",
      "Epoch 94 \t Training Loss: 0.703600\n",
      "Epoch 95 \t Training Loss: 0.703202\n",
      "Epoch 96 \t Training Loss: 0.702079\n",
      "Epoch 97 \t Training Loss: 0.701186\n",
      "Epoch 98 \t Training Loss: 0.706495\n",
      "Epoch 99 \t Training Loss: 0.703194\n",
      "Epoch 100 \t Training Loss: 0.698692\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in training_generator:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch %d \\t Training Loss: %.6f' % (epoch + 1, running_loss/len(training_generator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Evaluate(model, testing_generator):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testing_generator:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            _, labels = torch.max(labels, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    print('Accuracy Recurrent Neural Networks: %.6f' % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Recurrent Neural Networks: 0.623833\n"
     ]
    }
   ],
   "source": [
    "Model_Evaluate(model, testing_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models?\n",
    "\n",
    "The Recurrent Neural Network (RNN) model is lower than the accuracy of the Feedforward Neural Network (FNN) models. The FNN models achieve an accuracy of 0.660667 using the averaging method. In contrast, the Recurrent Neural Network (RNN) model achieves a higher accuracy (0.623833) compared to the Feedforward Neural Network (FNN) model that uses concatenation method (accuracy of 0.562333).\n",
    "\n",
    "However, it's important to note that the effectiveness of different models can vary depending on the task and dataset, and it's possible that the RNN model may perform better on other sentiment analysis tasks or datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b) - Recurrent Neural Networks (GRU cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture with GRU cell\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        output, _ = self.gru(x, h0)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "input_size = 300 # 20 * 300\n",
    "hidden_size = 20\n",
    "num_layers = 1\n",
    "output_size = 3\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU(\n",
      "  (gru): GRU(300, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = GRU(input_size, hidden_size, num_layers, output_size)\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t Training Loss: 0.957988\n",
      "Epoch 2 \t Training Loss: 0.811878\n",
      "Epoch 3 \t Training Loss: 0.776203\n",
      "Epoch 4 \t Training Loss: 0.756927\n",
      "Epoch 5 \t Training Loss: 0.741676\n",
      "Epoch 6 \t Training Loss: 0.729803\n",
      "Epoch 7 \t Training Loss: 0.718652\n",
      "Epoch 8 \t Training Loss: 0.708747\n",
      "Epoch 9 \t Training Loss: 0.700041\n",
      "Epoch 10 \t Training Loss: 0.692355\n",
      "Epoch 11 \t Training Loss: 0.685226\n",
      "Epoch 12 \t Training Loss: 0.677274\n",
      "Epoch 13 \t Training Loss: 0.669224\n",
      "Epoch 14 \t Training Loss: 0.664576\n",
      "Epoch 15 \t Training Loss: 0.657392\n",
      "Epoch 16 \t Training Loss: 0.650452\n",
      "Epoch 17 \t Training Loss: 0.645329\n",
      "Epoch 18 \t Training Loss: 0.639979\n",
      "Epoch 19 \t Training Loss: 0.634853\n",
      "Epoch 20 \t Training Loss: 0.628883\n",
      "Epoch 21 \t Training Loss: 0.625003\n",
      "Epoch 22 \t Training Loss: 0.619841\n",
      "Epoch 23 \t Training Loss: 0.613468\n",
      "Epoch 24 \t Training Loss: 0.610178\n",
      "Epoch 25 \t Training Loss: 0.604051\n",
      "Epoch 26 \t Training Loss: 0.600960\n",
      "Epoch 27 \t Training Loss: 0.595957\n",
      "Epoch 28 \t Training Loss: 0.592083\n",
      "Epoch 29 \t Training Loss: 0.586774\n",
      "Epoch 30 \t Training Loss: 0.583204\n",
      "Epoch 31 \t Training Loss: 0.579907\n",
      "Epoch 32 \t Training Loss: 0.573618\n",
      "Epoch 33 \t Training Loss: 0.570702\n",
      "Epoch 34 \t Training Loss: 0.566138\n",
      "Epoch 35 \t Training Loss: 0.565185\n",
      "Epoch 36 \t Training Loss: 0.557722\n",
      "Epoch 37 \t Training Loss: 0.555917\n",
      "Epoch 38 \t Training Loss: 0.550937\n",
      "Epoch 39 \t Training Loss: 0.547399\n",
      "Epoch 40 \t Training Loss: 0.543049\n",
      "Epoch 41 \t Training Loss: 0.539767\n",
      "Epoch 42 \t Training Loss: 0.537474\n",
      "Epoch 43 \t Training Loss: 0.533748\n",
      "Epoch 44 \t Training Loss: 0.528361\n",
      "Epoch 45 \t Training Loss: 0.525844\n",
      "Epoch 46 \t Training Loss: 0.521466\n",
      "Epoch 47 \t Training Loss: 0.519077\n",
      "Epoch 48 \t Training Loss: 0.515237\n",
      "Epoch 49 \t Training Loss: 0.510918\n",
      "Epoch 50 \t Training Loss: 0.509838\n",
      "Epoch 51 \t Training Loss: 0.505732\n",
      "Epoch 52 \t Training Loss: 0.501264\n",
      "Epoch 53 \t Training Loss: 0.498677\n",
      "Epoch 54 \t Training Loss: 0.495516\n",
      "Epoch 55 \t Training Loss: 0.492651\n",
      "Epoch 56 \t Training Loss: 0.492592\n",
      "Epoch 57 \t Training Loss: 0.488120\n",
      "Epoch 58 \t Training Loss: 0.484016\n",
      "Epoch 59 \t Training Loss: 0.482403\n",
      "Epoch 60 \t Training Loss: 0.479262\n",
      "Epoch 61 \t Training Loss: 0.475009\n",
      "Epoch 62 \t Training Loss: 0.473592\n",
      "Epoch 63 \t Training Loss: 0.468755\n",
      "Epoch 64 \t Training Loss: 0.467171\n",
      "Epoch 65 \t Training Loss: 0.465142\n",
      "Epoch 66 \t Training Loss: 0.462384\n",
      "Epoch 67 \t Training Loss: 0.460089\n",
      "Epoch 68 \t Training Loss: 0.462849\n",
      "Epoch 69 \t Training Loss: 0.458796\n",
      "Epoch 70 \t Training Loss: 0.455339\n",
      "Epoch 71 \t Training Loss: 0.451939\n",
      "Epoch 72 \t Training Loss: 0.451605\n",
      "Epoch 73 \t Training Loss: 0.447317\n",
      "Epoch 74 \t Training Loss: 0.445424\n",
      "Epoch 75 \t Training Loss: 0.443188\n",
      "Epoch 76 \t Training Loss: 0.438841\n",
      "Epoch 77 \t Training Loss: 0.442507\n",
      "Epoch 78 \t Training Loss: 0.435662\n",
      "Epoch 79 \t Training Loss: 0.433615\n",
      "Epoch 80 \t Training Loss: 0.429910\n",
      "Epoch 81 \t Training Loss: 0.430357\n",
      "Epoch 82 \t Training Loss: 0.430183\n",
      "Epoch 83 \t Training Loss: 0.424403\n",
      "Epoch 84 \t Training Loss: 0.423044\n",
      "Epoch 85 \t Training Loss: 0.421001\n",
      "Epoch 86 \t Training Loss: 0.422558\n",
      "Epoch 87 \t Training Loss: 0.418164\n",
      "Epoch 88 \t Training Loss: 0.419339\n",
      "Epoch 89 \t Training Loss: 0.418565\n",
      "Epoch 90 \t Training Loss: 0.413254\n",
      "Epoch 91 \t Training Loss: 0.408127\n",
      "Epoch 92 \t Training Loss: 0.409080\n",
      "Epoch 93 \t Training Loss: 0.412083\n",
      "Epoch 94 \t Training Loss: 0.406501\n",
      "Epoch 95 \t Training Loss: 0.406538\n",
      "Epoch 96 \t Training Loss: 0.402153\n",
      "Epoch 97 \t Training Loss: 0.398575\n",
      "Epoch 98 \t Training Loss: 0.402369\n",
      "Epoch 99 \t Training Loss: 0.399033\n",
      "Epoch 100 \t Training Loss: 0.394391\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in training_generator:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch %d \\t Training Loss: %.6f' % (epoch + 1, running_loss/len(training_generator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Evaluate(model, testing_generator):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testing_generator:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            _, labels = torch.max(labels, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    print('Accuracy Recurrent Neural Networks (Gated Recurrent unit cell): %.6f' % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Recurrent Neural Networks (Gated Recurrent unit cell): 0.614833\n"
     ]
    }
   ],
   "source": [
    "Model_Evaluate(model, testing_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models?\n",
    "\n",
    "The Gated Recurrent Unit (GRU) model achieves a higher accuracy (0.614833) compared to the Feedforward Neural Network (FNN) model that uses the concatenation method (accuracy of 0.562333). However, the FNN model that uses the average method achieves a higher accuracy of 0.660667 compared to both the GRU and concatenation-based FNN models. However, as previously mentioned, the effectiveness of different models can vary depending on the task and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c) - Recurrent Neural Networks (LSTM unit cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM neural network architecture\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        output, _ = self.lstm(x, (h0, c0))\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "input_size = 300 # 20 * 300\n",
    "hidden_size = 20\n",
    "num_layers = 1\n",
    "output_size = 3\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(300, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t Training Loss: 0.955152\n",
      "Epoch 2 \t Training Loss: 0.836005\n",
      "Epoch 3 \t Training Loss: 0.800330\n",
      "Epoch 4 \t Training Loss: 0.777768\n",
      "Epoch 5 \t Training Loss: 0.758508\n",
      "Epoch 6 \t Training Loss: 0.745380\n",
      "Epoch 7 \t Training Loss: 0.733737\n",
      "Epoch 8 \t Training Loss: 0.722880\n",
      "Epoch 9 \t Training Loss: 0.713477\n",
      "Epoch 10 \t Training Loss: 0.704263\n",
      "Epoch 11 \t Training Loss: 0.693429\n",
      "Epoch 12 \t Training Loss: 0.688016\n",
      "Epoch 13 \t Training Loss: 0.680503\n",
      "Epoch 14 \t Training Loss: 0.672582\n",
      "Epoch 15 \t Training Loss: 0.665070\n",
      "Epoch 16 \t Training Loss: 0.659323\n",
      "Epoch 17 \t Training Loss: 0.651528\n",
      "Epoch 18 \t Training Loss: 0.644219\n",
      "Epoch 19 \t Training Loss: 0.638402\n",
      "Epoch 20 \t Training Loss: 0.631826\n",
      "Epoch 21 \t Training Loss: 0.627155\n",
      "Epoch 22 \t Training Loss: 0.620642\n",
      "Epoch 23 \t Training Loss: 0.614215\n",
      "Epoch 24 \t Training Loss: 0.608928\n",
      "Epoch 25 \t Training Loss: 0.604388\n",
      "Epoch 26 \t Training Loss: 0.598268\n",
      "Epoch 27 \t Training Loss: 0.593018\n",
      "Epoch 28 \t Training Loss: 0.586868\n",
      "Epoch 29 \t Training Loss: 0.582748\n",
      "Epoch 30 \t Training Loss: 0.576093\n",
      "Epoch 31 \t Training Loss: 0.572630\n",
      "Epoch 32 \t Training Loss: 0.565219\n",
      "Epoch 33 \t Training Loss: 0.562517\n",
      "Epoch 34 \t Training Loss: 0.557180\n",
      "Epoch 35 \t Training Loss: 0.552393\n",
      "Epoch 36 \t Training Loss: 0.548276\n",
      "Epoch 37 \t Training Loss: 0.541716\n",
      "Epoch 38 \t Training Loss: 0.537752\n",
      "Epoch 39 \t Training Loss: 0.534650\n",
      "Epoch 40 \t Training Loss: 0.530412\n",
      "Epoch 41 \t Training Loss: 0.524608\n",
      "Epoch 42 \t Training Loss: 0.520337\n",
      "Epoch 43 \t Training Loss: 0.519939\n",
      "Epoch 44 \t Training Loss: 0.511880\n",
      "Epoch 45 \t Training Loss: 0.510461\n",
      "Epoch 46 \t Training Loss: 0.504140\n",
      "Epoch 47 \t Training Loss: 0.500175\n",
      "Epoch 48 \t Training Loss: 0.496463\n",
      "Epoch 49 \t Training Loss: 0.494161\n",
      "Epoch 50 \t Training Loss: 0.489503\n",
      "Epoch 51 \t Training Loss: 0.488891\n",
      "Epoch 52 \t Training Loss: 0.484351\n",
      "Epoch 53 \t Training Loss: 0.481915\n",
      "Epoch 54 \t Training Loss: 0.477115\n",
      "Epoch 55 \t Training Loss: 0.472646\n",
      "Epoch 56 \t Training Loss: 0.471841\n",
      "Epoch 57 \t Training Loss: 0.468486\n",
      "Epoch 58 \t Training Loss: 0.463189\n",
      "Epoch 59 \t Training Loss: 0.457424\n",
      "Epoch 60 \t Training Loss: 0.456549\n",
      "Epoch 61 \t Training Loss: 0.455135\n",
      "Epoch 62 \t Training Loss: 0.453537\n",
      "Epoch 63 \t Training Loss: 0.444415\n",
      "Epoch 64 \t Training Loss: 0.445547\n",
      "Epoch 65 \t Training Loss: 0.441859\n",
      "Epoch 66 \t Training Loss: 0.439820\n",
      "Epoch 67 \t Training Loss: 0.439644\n",
      "Epoch 68 \t Training Loss: 0.432544\n",
      "Epoch 69 \t Training Loss: 0.431664\n",
      "Epoch 70 \t Training Loss: 0.427088\n",
      "Epoch 71 \t Training Loss: 0.420421\n",
      "Epoch 72 \t Training Loss: 0.426360\n",
      "Epoch 73 \t Training Loss: 0.416108\n",
      "Epoch 74 \t Training Loss: 0.418398\n",
      "Epoch 75 \t Training Loss: 0.415948\n",
      "Epoch 76 \t Training Loss: 0.411684\n",
      "Epoch 77 \t Training Loss: 0.407255\n",
      "Epoch 78 \t Training Loss: 0.410648\n",
      "Epoch 79 \t Training Loss: 0.404858\n",
      "Epoch 80 \t Training Loss: 0.401680\n",
      "Epoch 81 \t Training Loss: 0.397976\n",
      "Epoch 82 \t Training Loss: 0.398328\n",
      "Epoch 83 \t Training Loss: 0.391327\n",
      "Epoch 84 \t Training Loss: 0.390739\n",
      "Epoch 85 \t Training Loss: 0.389554\n",
      "Epoch 86 \t Training Loss: 0.389599\n",
      "Epoch 87 \t Training Loss: 0.383531\n",
      "Epoch 88 \t Training Loss: 0.380884\n",
      "Epoch 89 \t Training Loss: 0.381205\n",
      "Epoch 90 \t Training Loss: 0.379850\n",
      "Epoch 91 \t Training Loss: 0.376009\n",
      "Epoch 92 \t Training Loss: 0.377902\n",
      "Epoch 93 \t Training Loss: 0.373447\n",
      "Epoch 94 \t Training Loss: 0.370378\n",
      "Epoch 95 \t Training Loss: 0.366742\n",
      "Epoch 96 \t Training Loss: 0.370829\n",
      "Epoch 97 \t Training Loss: 0.362051\n",
      "Epoch 98 \t Training Loss: 0.357922\n",
      "Epoch 99 \t Training Loss: 0.362277\n",
      "Epoch 100 \t Training Loss: 0.357349\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in training_generator:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch %d \\t Training Loss: %.6f' % (epoch + 1, running_loss/len(training_generator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Evaluate(model, testing_generator):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testing_generator:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    print('Accuracy Recurrent Neural Networks (LSTM unit cell): %.6f' % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Recurrent Neural Networks (LSTM unit cell): 0.626667\n"
     ]
    }
   ],
   "source": [
    "Model_Evaluate(model, testing_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do you conclude by comparing accuracy values you obtain by GRU, LSTM, and simple RNN?\n",
    "\n",
    "The LSTM outperforms GRU and simple RNN in terms of accuracy. The accuracy of the simple RNN model is the lowest among the three, while the accuracy of the GRU model is slightly lower than that of the LSTM model. \n",
    "\n",
    "LSTM and GRU are more advanced RNN architectures that are specifically designed to address the vanishing gradient problem that can occur in simple RNNs. They have additional mechanisms such as gates that allow them to selectively retain or forget information from past inputs, which can be very helpful in processing long sequences of data. This often leads to better performance compared to simple RNNs. In the case of the specific task you are working on, it seems that LSTM was able to outperform both GRU and simple RNN, possibly because it was better able to capture the complex relationships between the text inputs and the corresponding labels.\n",
    "\n",
    "This suggests that the LSTM model is better at capturing long-term dependencies and relationships between words in the input text, which is important for sentiment analysis tasks."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TqW3bt0Hb7Kk"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
